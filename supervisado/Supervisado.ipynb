{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Práctico de Aprendizaje Supervisado**\n",
    "\n",
    "La idea general de este práctico es profundizar sobre los modelos desarrollados en el práctico anterior. Además de buscar mejorar las predicciones, vamos a hacer énfasis en las bondades y limitaciones de cada modelo, como para intentar comprender qué hace que ciertos modelos se desempeñen mejor que otros. \n",
    "\n",
    "### **Modelos de ensamble**\n",
    "\n",
    "Los árboles de decisión suelen ser buenos modelos para trabajar con datos categóricos. Como nuestros datos son mayoritariamente de este tipo, vamos a estudiar un poco más en detalle esta familia de modelos. En este práctico vamos a emplear dos generalizaciones de árboles de decisión: [RandomForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) y [XGBoost](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier). Para este último, tienen que installar el paquete xgboost, que lo pueden hacer con ``pip install xgboost``. Estos dos modelos utilizan dos técnicas de ensamble: bagging y boosting, respectivamente, y cada uno de ellos tiene pros y contras. Aunque seguramente lo vieron en los teóricos, les recomiendo mirar los videos relacionados de [StatQuest](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&ab_channel=StatQuestwithJoshStarmer), que son bastante amenos.\n",
    "\n",
    "Para este práctico tendrán que entrenar ambos modelos y comparar los resultados con los obtenidos empleando un único árbol de decisión. Para darle robustez a los resultados, ahora utilizaremos [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html) en los datos de entrenamiento (usen entre 3 y 5 divisones). \n",
    "\n",
    "\n",
    "### **Ajuste de hiperparámetros**\n",
    "\n",
    "Cada modelo tiene un conjunto de hiperparámetros que es necesario definir antes de entrenarlos. La cantidad de estos hiperparámetros depende mucho de la familia de modelos. Los modelos bayesianos, por ejemplo, suelen tener muy pocos, mientras que las redes neuronales pueden tener varios millones. \n",
    "\n",
    "En este práctico tendrán que realizar una búsqueda exhaustiva de hiperparámetros para los modelos elegidos. Como este proceso puede llegar a ser muy costoso computacionalmente, es importante que usen los insights que hayan obtenido en el práctico anterior para decidir qué hiperparámetros vale la pena variar, qué rangos de valores podría tomar, y cómo debería ser la variación (lineal, logarítmica). Para hacer esta búsqueda, utilicen al menos uno de los siguientes [métodos](https://towardsdatascience.com/hyperparameter-optimization-with-scikit-learn-scikit-opt-and-keras-f13367f3e796):\n",
    "\n",
    "- Grid search\n",
    "\n",
    "- Random search\n",
    "\n",
    "- Bayesian optimization\n",
    "\n",
    "**Opcional:** Si usan más de uno, comenten las ventajas y desventajas de cada método.\n",
    "\n",
    "\n",
    "### **Feature importance**\n",
    "\n",
    "En general, los features elegidos pueden ser más o menos informativos. Puede que algunos sean extremadamente útiles para realizar predicciones, mientras que otros no tengan ningún tipo de correlación con la variable objetivo. Incluso, algunos features, o combinaciones de features, pueden disminuir la performance de un modelo. Hay varios métodos para calcular la importancia de los features (varios de ellos descriptos [acá](https://machinelearningmastery.com/calculate-feature-importance-with-python/). Algunos métodos dependen del modelo en cuestión, y otros se pueden aplicar a todos los modelos. Dentro de estos últimos está el método que vamos a usar en este práctico, llamado [permutation feature importance](https://scikit-learn.org/stable/modules/permutation_importance.html). Básicamente, este método consiste en ver qué tanto varía la performance de un modelo si tomamos de a uno cada feature y lo randomizamos.\n",
    "\n",
    "Lo que deberán hacer es calcular la importancia de los features utilizados, determinar cuáles de ellos son los más informativos y discutir sobre el resultado obtenido. Por otro lado, evalúen cómo varía el poder predictivo del modelo si remueven los features menos importantes. \n",
    "\n",
    "**Opcional:** Cuando trabajen con árboles de decisión, van a ver que los modelos de ``sklearn`` tienen un atributo llamado ``.feature_importances_``. En este [link](https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e) hay una discusión sobre los pros y contras de este método, comparado con permutation feature importance. Si quieren, determinen la importancia de las dos maneras y evalúen cuál de las dos resulta más razonable.\n",
    "\n",
    "### **Opcional: Feature engineering**\n",
    "\n",
    "Trabajar con modelos complejos y ajustar hiperparámetros hace que los cálculos sean bastante pesados. Para acotar los tiempos, pueden dedicar un poco más de esfuerzo a la etapa de feature engineering. Acá hay varias cosas que se pueden hacer. Una de ellas ya la mencionamos en el apartado anterior (eliminar features con poca importancia). Otras opciones pueden ser:\n",
    "\n",
    "- Utilizar Label Encoding en lugar de One-Hot Encoding en las variables categóricas, particularmente en aquellas que tengan varias categorías. Usar Label Encoding para una variable categórica no ordinal puede parecer inadecuado, ya que estamos incorporando una relación ordinal ficticia. Como contraparte, esto nos puede dar una ventaja computacional muy grande. Al reducir el número de features, vamos a poder correr más instancias del modelo, con lo que podríamos explorar más el espacio de hiperparámetros, por ejemplo. Esto puede resultar en un modelo mejor que si hubiésemos utilizado One-Hot Encoding.\n",
    "\n",
    "- Utilizar modelos con features incrementales. La idea acá es arrancar con un modelo con muy pocos features, los que consideren más relevantes. A partir de ese modelo, ir incorporando de a uno nuevos features y evaluar si el modelo mejora o no. Si no hay una mejora significativa, descartar el feature, de manera tal de mantener el modelo fácil de entrenar.\n",
    "\n",
    "\n",
    "### **Opcional: Modelos no basados en árboles**\n",
    "\n",
    "A modo de comparación, pueden repetir lo anterior para algún modelo que no esté basado en árboles. Pueden usar modelos lineales o no lineales, los que quieran. Elijan uno que les parezca más apropiado y busquen maximizar la performance con una búsqueda exhaustiva de hiperparámetros. \n",
    "\n",
    "\n",
    "### **Notas**\n",
    "\n",
    "Para caracterizar el desempeño de los modelos deberán elegir, al igual que en el práctico anterior, una métrica. Pueden utilizar la misma que usaron antes o utilizar una nueva, pero una vez que la elijan, mantengan siempre la misma en todo el proceso.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ausentismo] *",
   "language": "python",
   "name": "conda-env-ausentismo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
